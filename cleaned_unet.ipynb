{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9acb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything, LightningModule, Trainer\n",
    "import multiprocessing\n",
    "import torchmetrics\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"cityscapes_format_animal_gta/gtFine/val/aachen/\"\n",
    "for f in os.listdir(base_path):\n",
    "    c = f.replace(\"__\",\"_\")\n",
    "    os.rename(os.path.join(base_path,f),os.path.join(base_path,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Cityscapes('cityscapes_format_animal_gta/', split='train', mode='fine',target_type='semantic')\n",
    "fig,ax=plt.subplots(ncols=2,figsize=(12,8))\n",
    "ax[0].imshow(dataset[10][0])\n",
    "ax[1].imshow(dataset[10][1],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5375ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('class_dict_seg.txt')\n",
    "classes = df['name']\n",
    "palette = df[['r', 'g', 'b']].values\n",
    "id2label = classes.to_dict()\n",
    "label2id = {k:list(v) for k, v in enumerate(palette)}\n",
    "label_colours = label2id\n",
    "label_colours\n",
    "n_classes= len(label_colours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ddc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [   [  0,   0,   0],\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [0, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32],\n",
    "    ]\n",
    "\n",
    "label_colours = dict(zip(range(n_classes), colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_segmap(mask):\n",
    "\n",
    "    if mask.shape[-1] ==3:\n",
    "        return mask[:,:,0]\n",
    "    else:\n",
    "        return mask\n",
    "    return mask[:,:,0]\n",
    "\n",
    "def decode_segmap(temp):\n",
    "    temp=temp.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, n_classes):\n",
    "        r[temp == l] = label_colours[l][0]\n",
    "        g[temp == l] = label_colours[l][1]\n",
    "        b[temp == l] = label_colours[l][2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:, :, 0] = r / 255.0\n",
    "    rgb[:, :, 1] = g / 255.0\n",
    "    rgb[:, :, 2] = b / 255.0\n",
    "    return rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=A.Compose(\n",
    "[\n",
    "    A.Resize(256, 512),\n",
    "    A.HorizontalFlip(),\n",
    "    ToTensorV2(),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tranform_Data(Cityscapes):\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "\n",
    "        targets: Any = []\n",
    "        for i, t in enumerate(self.target_type):\n",
    "            target = Image.open(self.targets[index][i])\n",
    "            targets.append(target)\n",
    "            \n",
    "        target = tuple(targets) if len(targets) > 1 else targets[0]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed=transform(image=np.array(image), mask=np.array(target))            \n",
    "        return transformed['image'],transformed['mask'][:,:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Tranform_Data('cityscapes_format_animal_gta/', split='val', mode='fine',target_type='semantic',transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(LightningModule):\n",
    "  def __init__(self):\n",
    "    super(UNet,self).__init__()\n",
    "    #architecute\n",
    "    self.layer = smp.Unet(\n",
    "                encoder_name=\"resnet34\",      \n",
    "                encoder_weights=\"imagenet\",     \n",
    "                in_channels=3,                  \n",
    "                classes=n_classes,                    \n",
    "            )\n",
    "\n",
    "    self.lr=1e-3\n",
    "    self.batch_size=16\n",
    "    self.numworker=multiprocessing.cpu_count()//4\n",
    "\n",
    "\n",
    "    self.criterion= smp.losses.DiceLoss(mode='multiclass')\n",
    "    self.metrics = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=n_classes)\n",
    "    \n",
    "    self.train_class = Tranform_Data('cityscapes_format_animal_gta/', split='train', mode='fine',\n",
    "                     target_type='semantic',transforms=transform)\n",
    "    self.val_class = Tranform_Data('cityscapes_format_animal_gta/', split='val', mode='fine',\n",
    "                     target_type='semantic',transforms=transform)\n",
    "    \n",
    "    \n",
    "  def process(self,image,segment):\n",
    "    out=self(image)\n",
    "    segment=encode_segmap(segment)\n",
    "    loss=self.criterion(out,segment.long())\n",
    "    iou=self.metrics(out,segment)\n",
    "    return loss,iou\n",
    "    \n",
    "  def forward(self,x):\n",
    "    return self.layer(x)\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    opt=torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "    return opt\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train_class, batch_size=self.batch_size, \n",
    "                      shuffle=True,pin_memory=True,num_workers=self.numworker)\n",
    "\n",
    "  def training_step(self,batch,batch_idx):\n",
    "    image,segment=batch\n",
    "    loss,iou=self.process(image,segment)\n",
    "    self.log('train_loss', loss,on_step=False, on_epoch=True,prog_bar=True)\n",
    "    self.log('train_iou', iou,on_step=False, on_epoch=True,prog_bar=False)\n",
    "    return loss\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.val_class, batch_size=self.batch_size, \n",
    "                      shuffle=False,pin_memory=True,num_workers=self.numworker)\n",
    "    \n",
    "  def validation_step(self,batch,batch_idx):\n",
    "    image,segment=batch\n",
    "    loss,iou=self.process(image,segment)\n",
    "    self.log('val_loss', loss,on_step=False, on_epoch=True,prog_bar=False)\n",
    "    self.log('val_iou', iou,on_step=False, on_epoch=True,prog_bar=False)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f81695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',dirpath='checkpoints',filename='file',save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a98667",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SLURM_NTASKS']='1'\n",
    "#os.environ['SLURM_TASKS_PER_NODE']='16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410724e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = Trainer(max_epochs=10,precision=16,callbacks=[checkpoint_callback],num_nodes=1)\n",
    "trainer = Trainer(max_epochs=50,callbacks=[checkpoint_callback],devices=1, accelerator=\"gpu\",num_nodes=1,precision='32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0575f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ce505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'unet_gta.pth')\n",
    "model.load_state_dict(torch.load('unet_gta.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class = Tranform_Data('cityscapes_format_animal_gta/', split='val', mode='fine',\n",
    "                     target_type='semantic',transforms=transform)\n",
    "test_loader=DataLoader(test_class, batch_size=12, \n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c801462",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        img,seg=batch\n",
    "        output=model(img.cuda())\n",
    "        break\n",
    "print(img.shape,seg.shape,output.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample=6\n",
    "invimg=inv_normalize(img[sample])\n",
    "outputx=output.detach().cpu()[sample]\n",
    "encoded_mask=encode_segmap(seg[sample].clone()) \n",
    "decoded_mask=decode_segmap(encoded_mask.clone())  \n",
    "decoded_ouput=decode_segmap(torch.argmax(outputx,0))\n",
    "fig,ax=plt.subplots(ncols=3,figsize=(16,50),facecolor='white')  \n",
    "ax[0].imshow(np.moveaxis(invimg.numpy(),0,2))\n",
    "\n",
    "ax[1].imshow(decoded_mask) \n",
    "ax[2].imshow(decoded_ouput) \n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[2].axis('off')\n",
    "ax[0].set_title('Input Image')\n",
    "ax[1].set_title('Ground mask')\n",
    "ax[2].set_title('Predicted mask')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
